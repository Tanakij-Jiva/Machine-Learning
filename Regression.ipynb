{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 5: Regression"
      ],
      "metadata": {
        "id": "QIFhEZ8s6RDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our objective function now becomes:\n",
        "\n",
        "$$ J_{ridge}(Î˜, Î˜_{0}) = \\frac{1}{n} \\Sigma_{n}^{i=1}(Î˜^{T}x^{(i)} + Î˜_{0} - y^{(i)})^{2} + Î»||Î˜||^{2} $$\n",
        "\n",
        "which we use hypothesis $ h(x; Î˜, Î˜_{0}) = Î˜^{T}x + Î˜_{0} $ and squared loss, $ Loss(guess, actual) = (guess âˆ’ actual)^{2} $, in our case for now. The second term is the regulariser.\n",
        "\n",
        "Larger  ðœ†  values pressure  ðœƒ  values to be near zero. Note that we don't penalize $ ðœƒ_{0} $; intuitively, $ ðœƒ_{0} $ is what â€œfloats\" the regression surface to the right level for the data you have, and so you shouldn't make it harder to fit a data set where the  ð‘¦  values tend to be around one million than one where they tend to be around one. The other parameters control the orientation of the regression surface, and we prefer it to have a not-too-crazy orientation."
      ],
      "metadata": {
        "id": "jBZ66Clc6hw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our task is to find\n",
        "\n",
        "$$ Î˜^{*}, Î˜_{0}^{*} = argmin_{Î˜, Î˜_{0}} J(Î˜, Î˜_{0}) $$"
      ],
      "metadata": {
        "id": "i31DpSOi9xU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two ways of finding the optimal $ Î˜, Î˜_{0} $:"
      ],
      "metadata": {
        "id": "OFDyeED--YZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Analytical Solution: Ordinary Least Squares (OLS)"
      ],
      "metadata": {
        "id": "HGna1NXa-kWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ Î˜_{ridge} = (W^{T}W + nÎ»I)^{-1} W^{T}T $$\n",
        "\n",
        "where $ W = X^{T} $ and $ T = Y^{T} $.\n",
        "\n",
        "The inverse term becomes invertible when $ ðœ† > 0 $ .  "
      ],
      "metadata": {
        "id": "TaQoTieX-27j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Optimization via Gradient Descent"
      ],
      "metadata": {
        "id": "w2wEYbdU_ak3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gradient of the objective function is\n",
        "\n",
        "$$ âˆ‡_{Î˜}J = \\frac{2}{n} Î£_{i=1}^{n} (Î˜^{T}x^{(i)} + Î˜_{0} - y^{(i)})x^{(i)} + 2Î»Î˜ $$\n",
        "\n",
        "and partial derivative with respect to $ Î˜_{0} $\n",
        "\n",
        "$$ âˆ‡_{Î˜}J = \\frac{2}{n} Î£_{i=1}^{n} (Î˜^{T}x^{(i)} + Î˜_{0} - y^{(i)}) $$\n",
        "\n",
        "with these derivatives, we can do gradient descent, using the regular or stochastic gradient methods from chapter.\n",
        "\n",
        "Note that the objective functions for *OLS* and ridge regression are *convex*, which means they have only one minimum, which means, with a small enough step size, gradient descent is *guaranteed* to find the optimum."
      ],
      "metadata": {
        "id": "kP7y4KyO_jna"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKB7_QAj6OqY"
      },
      "outputs": [],
      "source": []
    }
  ]
}